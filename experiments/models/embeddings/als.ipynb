{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fffaee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: polars in c:\\important\\activities\\sirius\\.venv\\lib\\site-packages (1.35.2)\n",
      "Requirement already satisfied: pyarrow in c:\\important\\activities\\sirius\\.venv\\lib\\site-packages (22.0.0)\n",
      "Requirement already satisfied: numpy in c:\\important\\activities\\sirius\\.venv\\lib\\site-packages (2.3.5)\n",
      "Requirement already satisfied: tqdm in c:\\important\\activities\\sirius\\.venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\important\\activities\\sirius\\.venv\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: gensim in c:\\important\\activities\\sirius\\.venv\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: implicit in c:\\important\\activities\\sirius\\.venv\\lib\\site-packages (0.7.2)\n",
      "Requirement already satisfied: polars-runtime-32==1.35.2 in c:\\important\\activities\\sirius\\.venv\\lib\\site-packages (from polars) (1.35.2)\n",
      "Requirement already satisfied: colorama in c:\\important\\activities\\sirius\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\important\\activities\\sirius\\.venv\\lib\\site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\important\\activities\\sirius\\.venv\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\important\\activities\\sirius\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in c:\\important\\activities\\sirius\\.venv\\lib\\site-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: wrapt in c:\\important\\activities\\sirius\\.venv\\lib\\site-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install polars pyarrow numpy tqdm scikit-learn gensim implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c544b840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7ae1e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'raw_path': 'C:\\\\IMPORTANT\\\\ACTIVITIES\\\\sirius\\\\megamarket\\\\megamarket.parquet', 'out_dir': 'C:\\\\IMPORTANT\\\\ACTIVITIES\\\\sirius\\\\megamarket\\\\emb_out', 'read_limit_rows': None}\n"
     ]
    }
   ],
   "source": [
    "raw_path = r\"C:\\IMPORTANT\\ACTIVITIES\\sirius\\megamarket\\megamarket.parquet\"\n",
    "out_dir  = r\"C:\\IMPORTANT\\ACTIVITIES\\sirius\\megamarket\\emb_out\"\n",
    "read_limit_rows = None\n",
    "parquet_compression = \"zstd\"\n",
    "als_sampling = True\n",
    "min_user_events = 20\n",
    "min_item_users  = 20\n",
    "max_users_cap   = None\n",
    "max_items_cap   = None \n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "assert os.path.exists(raw_path), f\"Not found: {raw_path}\"\n",
    "print({\"raw_path\": raw_path, \"out_dir\": out_dir, \"read_limit_rows\": read_limit_rows})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56475b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_rows = pl.scan_parquet(raw_path).select(pl.count()).collect(engine=\"streaming\").item()\n",
    "#print(n_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2630cc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1724\\1521781038.py:2: PerformanceWarning: Resolving the schema of a LazyFrame is a potentially expensive operation. Use `LazyFrame.collect_schema()` to get the schema without this warning.\n",
      "  print(\"Columns:\", scan.schema)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: Schema([('user_id', Int32), ('datetime', Datetime(time_unit='ms', time_zone=None)), ('event', Int32), ('item_id', Int32), ('category_id', Int32), ('price', Float32)])\n",
      "Head shape: (5, 6)\n",
      "shape: (5, 6)\n",
      "┌─────────┬─────────────────────────┬───────┬─────────┬─────────────┬───────────┐\n",
      "│ user_id ┆ datetime                ┆ event ┆ item_id ┆ category_id ┆ price     │\n",
      "│ ---     ┆ ---                     ┆ ---   ┆ ---     ┆ ---         ┆ ---       │\n",
      "│ i32     ┆ datetime[ms]            ┆ i32   ┆ i32     ┆ i32         ┆ f32       │\n",
      "╞═════════╪═════════════════════════╪═══════╪═════════╪═════════════╪═══════════╡\n",
      "│ 1199174 ┆ 2023-02-13 03:12:19.131 ┆ 2     ┆ 1861088 ┆ 5395        ┆ -0.042062 │\n",
      "│ 3124963 ┆ 2023-02-13 09:38:55.674 ┆ 2     ┆ 2084441 ┆ 1531        ┆ 0.079817  │\n",
      "│ 2106055 ┆ 2023-02-13 08:26:05.004 ┆ 2     ┆ 2586689 ┆ 9294        ┆ -0.03899  │\n",
      "│ 4169844 ┆ 2023-02-13 16:55:08.470 ┆ 2     ┆ 2586689 ┆ 9294        ┆ -0.03899  │\n",
      "│ 7472260 ┆ 2023-02-13 03:30:13.093 ┆ 2     ┆ 2586689 ┆ 9294        ┆ -0.03899  │\n",
      "└─────────┴─────────────────────────┴───────┴─────────┴─────────────┴───────────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1724\\1521781038.py:8: PerformanceWarning: Determining the column names of a LazyFrame requires resolving its schema, which is a potentially expensive operation. Use `LazyFrame.collect_schema().names()` to get the column names without this warning.\n",
      "  cols = scan.columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time range: {'min_ts': [datetime.datetime(2023, 1, 15, 0, 0, 0, 708000)], 'max_ts': [datetime.datetime(2023, 4, 30, 23, 59, 59, 470000)]}\n",
      "Full dataset mode: using lazy pipelines (no base_df materialization).\n"
     ]
    }
   ],
   "source": [
    "scan = pl.scan_parquet(raw_path)\n",
    "print(\"Columns:\", scan.schema)\n",
    "\n",
    "head_df = scan.limit(5).collect(engine=\"streaming\")\n",
    "print(\"Head shape:\", head_df.shape)\n",
    "print(head_df)\n",
    "\n",
    "cols = scan.columns\n",
    "has_dt = \"datetime\" in cols\n",
    "\n",
    "if has_dt:\n",
    "    scan_filt = scan.filter(pl.col(\"datetime\").dt.month() != 5)\n",
    "    ts_df = scan_filt.select([\n",
    "        pl.col(\"datetime\").min().alias(\"min_ts\"),\n",
    "        pl.col(\"datetime\").max().alias(\"max_ts\"),\n",
    "    ]).collect(engine=\"streaming\")\n",
    "    print(\"Time range:\", ts_df.to_dict(as_series=False))\n",
    "else:\n",
    "    scan_filt = scan\n",
    "\n",
    "need = [c for c in [\"user_id\", \"item_id\", \"datetime\"] if c in cols]\n",
    "assert (\"user_id\" in need) and (\"item_id\" in need), \"Dataset must contain user_id and item_id\"\n",
    "base_scan = scan_filt.select(need)\n",
    "if read_limit_rows is not None:\n",
    "    base_df = base_scan.limit(read_limit_rows).collect(engine=\"streaming\")\n",
    "    print(\"Base slice shape:\", base_df.shape)\n",
    "    print(base_df.head(5))\n",
    "else:\n",
    "    base_df = None\n",
    "    print(\"Full dataset mode: using lazy pipelines (no base_df materialization).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2fd4f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_agg shape: (2521538, 6) -> C:\\IMPORTANT\\ACTIVITIES\\sirius\\megamarket\\emb_out\\user_agg.parquet\n",
      "shape: (5, 6)\n",
      "┌─────────┬────────┬─────────┬────────┬─────────────────────┬──────────────┐\n",
      "│ user_id ┆ events ┆ n_items ┆ n_days ┆ last_ts             ┆ recency_days │\n",
      "│ ---     ┆ ---    ┆ ---     ┆ ---    ┆ ---                 ┆ ---          │\n",
      "│ i32     ┆ u32    ┆ u32     ┆ u32    ┆ datetime[ms]        ┆ i64          │\n",
      "╞═════════╪════════╪═════════╪════════╪═════════════════════╪══════════════╡\n",
      "│ 9451783 ┆ 4      ┆ 4       ┆ 2      ┆ 2023-02-28 12:05:51 ┆ 61           │\n",
      "│ 9090022 ┆ 1      ┆ 1       ┆ 1      ┆ 2023-02-17 17:08:25 ┆ 72           │\n",
      "│ 4845635 ┆ 2      ┆ 2       ┆ 1      ┆ 2023-02-21 08:07:37 ┆ 68           │\n",
      "│ 3472058 ┆ 33     ┆ 29      ┆ 1      ┆ 2023-02-19 11:45:19 ┆ 70           │\n",
      "│ 1817437 ┆ 3      ┆ 3       ┆ 2      ┆ 2023-04-11 16:29:05 ┆ 19           │\n",
      "└─────────┴────────┴─────────┴────────┴─────────────────────┴──────────────┘\n"
     ]
    }
   ],
   "source": [
    "if read_limit_rows is not None:\n",
    "    _df = base_df\n",
    "    if has_dt and (\"datetime\" in _df.columns):\n",
    "        latest_ts = _df.select(pl.col(\"datetime\").max()).item()\n",
    "        df2 = _df.with_columns([\n",
    "            pl.col(\"datetime\").alias(\"ts\"),\n",
    "            pl.col(\"datetime\").dt.date().alias(\"date\"),\n",
    "        ])\n",
    "    else:\n",
    "        latest_ts = None\n",
    "        df2 = _df.with_columns([\n",
    "            pl.lit(None).alias(\"ts\"),\n",
    "            pl.lit(None).alias(\"date\"),\n",
    "        ])\n",
    "\n",
    "    user_agg = (\n",
    "        df2.group_by(\"user_id\", maintain_order=True).agg([\n",
    "            pl.len().alias(\"events\"),\n",
    "            pl.n_unique(\"item_id\").alias(\"n_items\"),\n",
    "            pl.n_unique(\"date\").alias(\"n_days\"),\n",
    "            pl.col(\"ts\").max().alias(\"last_ts\"),\n",
    "        ])\n",
    "    )\n",
    "    if latest_ts is not None:\n",
    "        user_agg = user_agg.with_columns(\n",
    "            (pl.lit(latest_ts) - pl.col(\"last_ts\")).dt.total_days().cast(pl.Int64).alias(\"recency_days\")\n",
    "        )\n",
    "    else:\n",
    "        user_agg = user_agg.with_columns(pl.lit(None).cast(pl.Int64).alias(\"recency_days\"))\n",
    "else:\n",
    "    latest_ts = scan_filt.select(pl.col(\"datetime\").max()).collect(engine=\"streaming\").item()\n",
    "    df2_lazy = base_scan.with_columns([\n",
    "        pl.col(\"datetime\").alias(\"ts\"),\n",
    "        pl.col(\"datetime\").dt.date().alias(\"date\"),\n",
    "    ]) if has_dt and (\"datetime\" in need) else base_scan.with_columns([\n",
    "        pl.lit(None).alias(\"ts\"),\n",
    "        pl.lit(None).alias(\"date\"),\n",
    "    ])\n",
    "    user_agg_lazy = (\n",
    "        df2_lazy.group_by(\"user_id\").agg([\n",
    "            pl.len().alias(\"events\"),\n",
    "            pl.n_unique(\"item_id\").alias(\"n_items\"),\n",
    "            pl.n_unique(\"date\").alias(\"n_days\"),\n",
    "            pl.col(\"ts\").max().alias(\"last_ts\"),\n",
    "        ])\n",
    "    )\n",
    "    user_out = os.path.join(out_dir, \"user_agg.parquet\")\n",
    "    user_agg_lazy.sink_parquet(user_out, compression=parquet_compression)\n",
    "    user_agg = pl.scan_parquet(user_out)\n",
    "    if latest_ts is not None:\n",
    "        user_agg = user_agg.with_columns((pl.lit(latest_ts) - pl.col(\"last_ts\")).dt.total_days().cast(pl.Int64).alias(\"recency_days\"))\n",
    "    else:\n",
    "        user_agg = user_agg.with_columns(pl.lit(None).cast(pl.Int64).alias(\"recency_days\"))\n",
    "    user_agg = user_agg.collect(engine=\"streaming\")\n",
    "    user_agg.write_parquet(user_out, compression=parquet_compression)\n",
    "    print(\"user_agg shape:\", user_agg.shape, \"->\", user_out)\n",
    "    print(user_agg.head(5))\n",
    "    if False:\n",
    "        pass\n",
    "\n",
    "if read_limit_rows is not None:\n",
    "    user_out = os.path.join(out_dir, \"user_agg.parquet\")\n",
    "    user_agg.write_parquet(user_out, compression=parquet_compression)\n",
    "    print(\"user_agg shape:\", user_agg.shape, \"->\", user_out)\n",
    "    print(user_agg.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43c420b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_agg shape: (3340545, 6) -> C:\\IMPORTANT\\ACTIVITIES\\sirius\\megamarket\\emb_out\\item_agg.parquet\n",
      "shape: (5, 6)\n",
      "┌─────────┬────────┬─────────┬─────────────────────┬─────────────────────────┬──────────────┐\n",
      "│ item_id ┆ events ┆ n_users ┆ avg_events_per_user ┆ last_ts                 ┆ recency_days │\n",
      "│ ---     ┆ ---    ┆ ---     ┆ ---                 ┆ ---                     ┆ ---          │\n",
      "│ i32     ┆ u32    ┆ u32     ┆ f64                 ┆ datetime[ms]            ┆ i64          │\n",
      "╞═════════╪════════╪═════════╪═════════════════════╪═════════════════════════╪══════════════╡\n",
      "│ 2575937 ┆ 1      ┆ 1       ┆ 1.0                 ┆ 2023-01-15 08:25:28.120 ┆ 105          │\n",
      "│ 2161189 ┆ 40     ┆ 12      ┆ 3.333333            ┆ 2023-04-30 15:33:15.491 ┆ 0            │\n",
      "│ 2012557 ┆ 19     ┆ 7       ┆ 2.714286            ┆ 2023-04-29 17:30:30     ┆ 1            │\n",
      "│ 3353771 ┆ 3      ┆ 2       ┆ 1.5                 ┆ 2023-03-25 23:11:26.647 ┆ 36           │\n",
      "│ 3353119 ┆ 16     ┆ 7       ┆ 2.285714            ┆ 2023-04-20 14:50:35.094 ┆ 10           │\n",
      "└─────────┴────────┴─────────┴─────────────────────┴─────────────────────────┴──────────────┘\n"
     ]
    }
   ],
   "source": [
    "if read_limit_rows is not None:\n",
    "    _df = base_df\n",
    "    if has_dt and (\"datetime\" in _df.columns):\n",
    "        latest_ts = _df.select(pl.col(\"datetime\").max()).item()\n",
    "        df2 = _df.with_columns([\n",
    "            pl.col(\"datetime\").alias(\"ts\"),\n",
    "        ])\n",
    "    else:\n",
    "        latest_ts = None\n",
    "        df2 = _df.with_columns([pl.lit(None).alias(\"ts\")])\n",
    "\n",
    "    item_agg = (\n",
    "        df2.group_by(\"item_id\", maintain_order=True).agg([\n",
    "            pl.len().alias(\"events\"),\n",
    "            pl.n_unique(\"user_id\").alias(\"n_users\"),\n",
    "            (pl.len() / pl.n_unique(\"user_id\")).cast(pl.Float64).alias(\"avg_events_per_user\"),\n",
    "            pl.col(\"ts\").max().alias(\"last_ts\"),\n",
    "        ])\n",
    "    )\n",
    "    if latest_ts is not None:\n",
    "        item_agg = item_agg.with_columns(\n",
    "            (pl.lit(latest_ts) - pl.col(\"last_ts\")).dt.total_days().cast(pl.Int64).alias(\"recency_days\")\n",
    "        )\n",
    "    else:\n",
    "        item_agg = item_agg.with_columns(pl.lit(None).cast(pl.Int64).alias(\"recency_days\"))\n",
    "else:\n",
    "    latest_ts = scan_filt.select(pl.col(\"datetime\").max()).collect(engine=\"streaming\").item()\n",
    "    df2_lazy = base_scan.with_columns([\n",
    "        pl.col(\"datetime\").alias(\"ts\")\n",
    "    ]) if has_dt and (\"datetime\" in need) else base_scan.with_columns([\n",
    "        pl.lit(None).alias(\"ts\")\n",
    "    ])\n",
    "    item_agg_lazy = (\n",
    "        df2_lazy.group_by(\"item_id\").agg([\n",
    "            pl.len().alias(\"events\"),\n",
    "            pl.n_unique(\"user_id\").alias(\"n_users\"),\n",
    "            (pl.len() / pl.n_unique(\"user_id\")).cast(pl.Float64).alias(\"avg_events_per_user\"),\n",
    "            pl.col(\"ts\").max().alias(\"last_ts\"),\n",
    "        ])\n",
    "    )\n",
    "    item_out = os.path.join(out_dir, \"item_agg.parquet\")\n",
    "    item_agg_lazy.sink_parquet(item_out, compression=parquet_compression)\n",
    "    item_agg = pl.scan_parquet(item_out)\n",
    "    if latest_ts is not None:\n",
    "        item_agg = item_agg.with_columns((pl.lit(latest_ts) - pl.col(\"last_ts\")).dt.total_days().cast(pl.Int64).alias(\"recency_days\"))\n",
    "    else:\n",
    "        item_agg = item_agg.with_columns(pl.lit(None).cast(pl.Int64).alias(\"recency_days\"))\n",
    "    item_agg = item_agg.collect(engine=\"streaming\")\n",
    "    item_agg.write_parquet(item_out, compression=parquet_compression)\n",
    "    print(\"item_agg shape:\", item_agg.shape, \"->\", item_out)\n",
    "    print(item_agg.head(5))\n",
    "    if False:\n",
    "        pass\n",
    "\n",
    "if read_limit_rows is not None:\n",
    "    item_out = os.path.join(out_dir, \"item_agg.parquet\")\n",
    "    item_agg.write_parquet(item_out, compression=parquet_compression)\n",
    "    print(\"item_agg shape:\", item_agg.shape, \"->\", item_out)\n",
    "    print(item_agg.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2153e556",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "import implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce543e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'interactions_rows': 35900269, 'num_users': 447207, 'num_items': 337637}\n"
     ]
    }
   ],
   "source": [
    "use_binary_weights = True\n",
    "\n",
    "if read_limit_rows is not None:\n",
    "    inter = (\n",
    "        base_df.select([\"user_id\", \"item_id\"]).group_by([\"user_id\", \"item_id\"], maintain_order=True)\n",
    "               .agg(pl.len().alias(\"cnt\"))\n",
    "    )\n",
    "else:\n",
    "    inter_lazy = base_scan.select([\"user_id\", \"item_id\"]).group_by([\"user_id\", \"item_id\"]).agg(pl.len().alias(\"cnt\"))\n",
    "    inter_out = os.path.join(out_dir, \"inter.parquet\")\n",
    "    inter_lazy.sink_parquet(inter_out, compression=parquet_compression)\n",
    "\n",
    "    if als_sampling:\n",
    "        inter_scan = pl.scan_parquet(inter_out)\n",
    "        user_cnts = inter_scan.group_by(\"user_id\").agg(pl.len().alias(\"u_events\"))\n",
    "        item_cnts = inter_scan.group_by(\"item_id\").agg(pl.len().alias(\"i_users\"))\n",
    "\n",
    "        active_users = user_cnts.filter(pl.col(\"u_events\") >= min_user_events).select(\"user_id\")\n",
    "        active_items = item_cnts.filter(pl.col(\"i_users\") >= min_item_users).select(\"item_id\")\n",
    "\n",
    "        inter_core_out = os.path.join(out_dir, \"inter_core.parquet\")\n",
    "        (inter_scan\n",
    "            .join(active_users, on=\"user_id\", how=\"inner\")\n",
    "            .join(active_items, on=\"item_id\", how=\"inner\")\n",
    "            .sink_parquet(inter_core_out, compression=parquet_compression)\n",
    "        )\n",
    "\n",
    "        inter_core_scan = pl.scan_parquet(inter_core_out)\n",
    "        if max_users_cap is not None:\n",
    "            keep_users = (inter_core_scan.select(\"user_id\").unique()\n",
    "                                         .limit(max_users_cap))\n",
    "            inter_core_scan = inter_core_scan.join(keep_users, on=\"user_id\", how=\"inner\")\n",
    "        if max_items_cap is not None:\n",
    "            keep_items = (inter_core_scan.select(\"item_id\").unique()\n",
    "                                          .limit(max_items_cap))\n",
    "            inter_core_scan = inter_core_scan.join(keep_items, on=\"item_id\", how=\"inner\")\n",
    "\n",
    "        inter = inter_core_scan.collect(engine=\"streaming\")\n",
    "    else:\n",
    "        inter = pl.scan_parquet(inter_out).collect(engine=\"streaming\")\n",
    "if use_binary_weights:\n",
    "    inter = inter.with_columns(pl.lit(1.0).alias(\"weight\"))\n",
    "else:\n",
    "    inter = inter.with_columns(pl.col(\"cnt\").cast(pl.Float32).alias(\"weight\"))\n",
    "\n",
    "uniq_users = inter.select(\"user_id\").unique().sort(\"user_id\").to_series().to_list()\n",
    "uniq_items = inter.select(\"item_id\").unique().sort(\"item_id\").to_series().to_list()\n",
    "user2idx = {u: i for i, u in enumerate(uniq_users)}\n",
    "item2idx = {it: i for i, it in enumerate(uniq_items)}\n",
    "\n",
    "user_arr = np.array(inter.select(\"user_id\").to_series().to_list(), dtype=np.int64)\n",
    "item_arr = np.array(inter.select(\"item_id\").to_series().to_list(), dtype=np.int64)\n",
    "rows = np.fromiter((user2idx[u] for u in user_arr), dtype=np.uint32, count=len(user_arr))\n",
    "cols = np.fromiter((item2idx[i] for i in item_arr), dtype=np.uint32, count=len(item_arr))\n",
    "data = np.array(inter.select(\"weight\").to_series().to_list(), dtype=np.float32)\n",
    "\n",
    "num_users = len(uniq_users)\n",
    "num_items = len(uniq_items)\n",
    "user_item_coo = coo_matrix((data, (rows, cols)), shape=(num_users, num_items), dtype=np.float32)\n",
    "user_item_csr = user_item_coo.tocsr()\n",
    "item_user_csr = user_item_csr.T.tocsr()\n",
    "\n",
    "print({\n",
    "    \"interactions_rows\": inter.height,\n",
    "    \"num_users\": num_users,\n",
    "    \"num_items\": num_items,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "823a9a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\IMPORTANT\\ACTIVITIES\\sirius\\.venv\\Lib\\site-packages\\implicit\\cpu\\als.py:95: RuntimeWarning: OpenBLAS is configured to use 16 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'OPENBLAS_NUM_THREADS=1' or by calling 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having OpenBLAS use a threadpool can lead to severe performance issues here.\n",
      "  check_blas_config()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a13dec089c54116bc2f0e85c116b563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"4\")\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"4\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"4\")\n",
    "\n",
    "factors = 48\n",
    "regularization = 0.01\n",
    "iterations = 12\n",
    "als = implicit.als.AlternatingLeastSquares(\n",
    "    factors=factors,\n",
    "    regularization=regularization,\n",
    "    iterations=iterations,\n",
    "    random_state=42,\n",
    ")\n",
    "als.fit(item_user_csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f165d1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS saved: C:\\IMPORTANT\\ACTIVITIES\\sirius\\megamarket\\emb_out\\als_user.parquet C:\\IMPORTANT\\ACTIVITIES\\sirius\\megamarket\\emb_out\\als_item.parquet\n"
     ]
    }
   ],
   "source": [
    "user_factors = als.user_factors\n",
    "item_factors = als.item_factors\n",
    "\n",
    "uf_rows, if_rows = user_factors.shape[0], item_factors.shape[0]\n",
    "nu, ni = len(uniq_users), len(uniq_items)\n",
    "\n",
    "if uf_rows == nu and if_rows == ni:\n",
    "    users_mat, items_mat = user_factors, item_factors\n",
    "elif uf_rows == ni and if_rows == nu:\n",
    "    users_mat, items_mat = item_factors, user_factors\n",
    "else:\n",
    "    raise RuntimeError(f\"ALS factor shapes do not match ids: user_factors={user_factors.shape}, item_factors={item_factors.shape}, users={nu}, items={ni}\")\n",
    "\n",
    "user_schema = [f\"f{i}\" for i in range(users_mat.shape[1])]\n",
    "item_schema = [f\"f{i}\" for i in range(items_mat.shape[1])]\n",
    "\n",
    "users_df = pl.DataFrame(users_mat, schema=user_schema)\n",
    "users_df = users_df.with_columns(pl.Series(\"user_id\", np.array(uniq_users, dtype=np.int64))) \\\n",
    "                   .select([\"user_id\", *user_schema])\n",
    "\n",
    "items_df = pl.DataFrame(items_mat, schema=item_schema)\n",
    "items_df = items_df.with_columns(pl.Series(\"item_id\", np.array(uniq_items, dtype=np.int64))) \\\n",
    "                   .select([\"item_id\", *item_schema])\n",
    "\n",
    "als_user_out = os.path.join(out_dir, \"als_user.parquet\")\n",
    "als_item_out = os.path.join(out_dir, \"als_item.parquet\")\n",
    "users_df.write_parquet(als_user_out)\n",
    "items_df.write_parquet(als_item_out)\n",
    "print(\"ALS saved:\", als_user_out, als_item_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
