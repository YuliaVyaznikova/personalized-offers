import pandas as pd
import gc
import pyarrow.parquet as pq
import os

# –ú–µ—Ç–æ–¥ 3: –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å –Ω–µ–º–µ–¥–ª–µ–Ω–Ω—ã–º —É–¥–∞–ª–µ–Ω–∏–µ–º
def process_incremental(file_path, product_id):
    """–ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å –Ω–µ–º–µ–¥–ª–µ–Ω–Ω—ã–º —É–¥–∞–ª–µ–Ω–∏–µ–º –Ω–µ–Ω—É–∂–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    print("\nüîÑ –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞...")

    required_columns = read_rc(product_id)
    
    # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª –ø–æ —á–∞—Å—Ç—è–º
    batch_size = 10000
    result_chunks = []
    
    # –°–æ–∑–¥–∞–µ–º ParquetFile –æ–±—ä–µ–∫—Ç
    pq_file = pq.ParquetFile(file_path)


    required_columns += ['timestamp', 'user_id', 'product_id', 'is_sold']

    for i, batch in enumerate(pq_file.iter_batches(batch_size=batch_size, columns=required_columns)):
        df_batch = batch.to_pandas()
        
        # –ù–µ–º–µ–¥–ª–µ–Ω–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        df_batch['timestamp'] = pd.to_datetime(df_batch['timestamp'])
        df_batch['year_month'] = df_batch['timestamp'].dt.strftime('%Y-%m')
        
        # –£–¥–∞–ª—è–µ–º –º–∞–π 2023
        df_batch = df_batch[df_batch['year_month'] != '2023-05']
        
        if len(df_batch) > 0:
            # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º —Ç–∏–ø—ã
            for col in df_batch.select_dtypes(include=['float64']).columns:
                df_batch[col] = df_batch[col].astype('float32')
            
            result_chunks.append(df_batch)
        
        # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞
        del df_batch
        gc.collect()
            
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    if result_chunks:
        result = pd.concat(result_chunks, ignore_index=True)
        #print(f"‚úÖ –ò—Ç–æ–≥–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä: {len(result):,} —Å—Ç—Ä–æ–∫")
        return result
    else:
        print("‚ö†Ô∏è  –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏")
        return pd.DataFrame()
    
import os
import pandas as pd

def read_rc(id_product):
    """–ß–∏—Ç–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –∏–∑ –∫–æ–ª–æ–Ω–∫–∏ dropped_feature —Ñ–∞–π–ª–∞ rfe_metrics_history_{id_product}"""
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –∏—Å—Ç–æ—Ä–∏–µ–π RFE
    file_path = os.path.join(
        project_root, 
        "utils", 
        "best_feature", 
        f"rfe_metrics_history_{id_product}.csv"
    )
    
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
    
    try:
        df = pd.read_csv(file_path)
        if 'dropped_feature' not in df.columns:
            raise ValueError(f"–ö–æ–ª–æ–Ω–∫–∞ 'dropped_feature' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —Ñ–∞–π–ª–µ {file_path}")
        
        dropped_features = df['dropped_feature'].dropna().astype(str).tolist()
        
        return dropped_features[-200:]
        
    except Exception as e:
        raise Exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ {file_path}: {str(e)}")


import pandas as pd
import pyarrow.parquet as pq
import gc

def process_last_month_all_products(file_path, use_columns):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä–æ–∫–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –º–µ—Å—è—Ü–∞ (OOT) –ø–æ –∫–∞–∂–¥–æ–º—É product_id,
    –ø—Ä–∏ —ç—Ç–æ–º —á–∏—Ç–∞–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ —É–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ use_columns.

    –ü—Ä–∏–º–µ—Ä:
        use_columns = ['user_id', 'product_id', 'timestamp', 'feature_101']
    """

    # timestamp –∏ product_id –Ω—É–∂–Ω—ã –≤—Å–µ–≥–¥–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –º–µ—Å—è—Ü–∞
    required_helper_cols = ['timestamp', 'product_id']
    read_cols = list(set(list(use_columns) + required_helper_cols))

    # ---------- –ü–µ—Ä–≤—ã–π –ø—Ä–æ—Ö–æ–¥: –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –º–µ—Å—è—Ü ----------
    pq_file = pq.ParquetFile(file_path)
    last_periods = {}

    for batch in pq_file.iter_batches(batch_size=50000, columns=read_cols):
        df = batch.to_pandas()

        df['timestamp'] = pd.to_datetime(df['timestamp'])
        df['year_month'] = df['timestamp'].dt.strftime('%Y-%m')

        #  –£–¥–∞–ª—è–µ–º –º–∞–π 2023
        df = df[df['year_month'] != '2023-05']
        if df.empty:
            continue

        df['period'] = df['timestamp'].dt.to_period('M')

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –º–µ—Å—è—Ü –¥–ª—è –∫–∞–∂–¥–æ–≥–æ product_id
        grouped = df.groupby('product_id')['period'].max()
        for pid, period in grouped.items():
            if pid not in last_periods:
                last_periods[pid] = period
            else:
                last_periods[pid] = max(last_periods[pid], period)

        del df, grouped
        gc.collect()

    # ---------- –í—Ç–æ—Ä–æ–π –ø—Ä–æ—Ö–æ–¥: —Å–æ–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω—é—é –¥–∞—Ç—É ----------
    pq_file = pq.ParquetFile(file_path)
    result_batches = []

    for batch in pq_file.iter_batches(batch_size=50000, columns=read_cols):
        df = batch.to_pandas()

        df['timestamp'] = pd.to_datetime(df['timestamp'])
        df['year_month'] = df['timestamp'].dt.strftime('%Y-%m')
        #  –£–¥–∞–ª—è–µ–º –º–∞–π 2023
        df = df[df['year_month'] != '2023-05']
        if df.empty:
            continue

        df['period'] = df['timestamp'].dt.to_period('M')

        # –ú–∞—Å–∫–∞ OOT
        mask = df.apply(
            lambda r: r['product_id'] in last_periods 
                      and r['period'] == last_periods[r['product_id']],
            axis=1
        )

        df_oot = df[mask]

        if not df_oot.empty:
            # –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ user-–≤—ã–±—Ä–∞–Ω–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã
            df_oot = df_oot[use_columns]
            result_batches.append(df_oot)

        del df, df_oot, mask
        gc.collect()

    if not result_batches:
        print("‚ö†Ô∏è –ù–µ—Ç OOT –¥–∞–Ω–Ω—ã—Ö!")
        return pd.DataFrame(columns=use_columns)

    df_final = pd.concat(result_batches, ignore_index=True)

    print(f"\nüî• –ò—Ç–æ–≥–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {len(df_final):,} —Å—Ç—Ä–æ–∫")
    return df_final
